# ASL-Detection
This is a project created for Programming in Python

# Why this project?
We all know that physically abled people like the deaf and dumb communicate using sign language. We wanted to build a system that can help bridge the gap with regard to communication between an average person that doesnt know sign language and a physically abled person

# About
- Our project focuses on developing a sign language recognition system using Python. This system can recognize American Sign Language (ASL) gestures and translate them into spoken or written language.
- Recognizing sign language gestures is challenging due to variations in hand shapes, motion, and facial expressions. Our system uses computer vision and machine learning algorithms, specifically a convolutional neural network (CNN) to achieve high accuracy rates. This technology has the potential to enhance communication between deaf and hearing individuals.

# Methodology
Dataset Creation, Input of Dataset and Training
- Read Data Frame by Frame
- Creating the Dataset
- Splitting The Dataset
- Training the CNN Model

# Methodology
Output Prediction Using Live Video Input
- Read Data Frame by Frame
- Creating A Threshold Image
- Comparison and Prediction
- Output Recognised Text

# Libraries Used
OpenCV(cv2), Numpy, Tensorflow, Keras, Scikit-Learn, Os, Sqlite3, Pyttsx3, Tkinter

# How to use the project?
- Create Histogram :
  Press 'C' to differentiate skin colour from background. Keep repeating this. Once satisfied, press 'S' to save Histogram
- Regognise Gesture :
  Open the Recognise Gesture option. The live camera now takes in input. Hold the needed gesture in the box for 10 frames and wait for the gesture to get recognised
- Fetch Output :
  Remove your hand from the box and now you will hear the automated voice speaking the output message

# Applications
Our sign language recognition system has potential applications in education, communication, and accessibility. It can be used to develop tools for teaching sign language to non-native speakers, improve communication between hearing and deaf individuals, and provide accessibility in public spaces, such as airports and hospitals.

# Conclusion
In conclusion, our sign language recognition project has demonstrated the potential of computer vision and machine learning techniques in recognizing American Sign Language gestures. Our system achieves high accuracy rates and has the potential to enhance communication and accessibility for individuals who are deaf or hard of hearing. Further research and development can lead to improved recognition accuracy and additional applications in education and accessibility.
